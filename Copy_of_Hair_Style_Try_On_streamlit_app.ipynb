{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asepindrak/hairstyle-try-on/blob/main/Copy_of_Hair_Style_Try_On_streamlit_app.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using GPU runtime type\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfRVilN3PhNf",
        "outputId": "6b826fe9-7e7a-4ba9-b429-dc39a7668ea3"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Oct 12 22:10:23 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   53C    P0              28W /  70W |    239MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Setup dependencies\n",
        "\n",
        "!git clone https://github.com/sanviiz/hairstyle-try-on.git\n",
        "%cd 'hairstyle-try-on'\n",
        "\n",
        "# download face segmentation model\n",
        "!wget https://is3.cloudhost.id/erp-sehati/AI/face_segment_checkpoints_256.pth.tar\n",
        "!mv face_segment_checkpoints_256.pth.tar ./image_segmentation/face_segment_checkpoints_256.pth.tar\n",
        "\n",
        "# download sdedit model\n",
        "!wget https://is3.cloudhost.id/erp-sehati/AI/celeba_hq.ckpt\n",
        "!mkdir checkpoints\n",
        "!mv celeba_hq.ckpt ./checkpoints/celeba_hq.ckpt\n",
        "\n",
        "# install librabies\n",
        "!pip -q install pyyaml \\\n",
        "tqdm \\\n",
        "mediapipe \\\n",
        "streamlit \\\n",
        "albumentations==0.4.6 \\\n",
        "pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rd_-o8cNOOAW",
        "outputId": "463b5ad4-fab5-4a43-c4ed-75c0b596ac22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'hairstyle-try-on'...\n",
            "remote: Enumerating objects: 280, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 280 (delta 8), reused 18 (delta 7), pack-reused 252 (from 1)\u001b[K\n",
            "Receiving objects: 100% (280/280), 13.33 MiB | 11.03 MiB/s, done.\n",
            "Resolving deltas: 100% (96/96), done.\n",
            "/content/hairstyle-try-on/hairstyle-try-on/hairstyle-try-on/hairstyle-try-on/hairstyle-try-on/hairstyle-try-on/hairstyle-try-on/hairstyle-try-on/hairstyle-try-on/hairstyle-try-on/hairstyle-try-on\n",
            "--2024-10-12 22:10:26--  https://is3.cloudhost.id/erp-sehati/AI/face_segment_checkpoints_256.pth.tar\n",
            "Resolving is3.cloudhost.id (is3.cloudhost.id)... 103.63.24.210, 103.63.24.211\n",
            "Connecting to is3.cloudhost.id (is3.cloudhost.id)|103.63.24.210|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 372579029 (355M) [application/octet-stream]\n",
            "Saving to: ‘face_segment_checkpoints_256.pth.tar’\n",
            "\n",
            "_256.pth.tar          6%[>                   ]  24.75M  --.-KB/s    eta 83m 29s"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title App.py\n",
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yaml\n",
        "import cv2\n",
        "import os\n",
        "import logging\n",
        "import traceback\n",
        "from image_segmentation.segment_inference import face_segment\n",
        "from runners.image_editing import Diffusion\n",
        "from image_landmark_transform.face_landmark import face_landmark_transform\n",
        "from image_artifact_fill.artifact_fill import face_artifact_fill\n",
        "from inference import resize_image, dict2namespace\n",
        "\n",
        "class app:\n",
        "    def __init__(self):\n",
        "        self.args = self.create_args()\n",
        "        self.init_app()\n",
        "        self.config = self.create_config()\n",
        "        run = st.button('RUN')\n",
        "        if self.args['target_image'] and self.args['source_image'] and run: # run pipeline when input images\n",
        "            self.pipeline()\n",
        "\n",
        "\n",
        "    def init_app(self, ):\n",
        "        st.title('Realistic Hairstyle try on')\n",
        "        st.subheader('Input images')\n",
        "        self.args['target_image'] = st.file_uploader('Target image (The person whose FACE you desire)',\n",
        "                                                type=['png', 'jpg', 'jpeg'])\n",
        "        self.args['source_image'] = st.file_uploader('Source image (The person whose HAIR you desire)',\n",
        "                                                type=['png', 'jpg', 'jpeg'])\n",
        "        # read original image\n",
        "        if self.args['target_image'] and self.args['source_image']:\n",
        "            self.target_image = self.read_image_from_streamlit(self.args['target_image'])\n",
        "            self.source_image = self.read_image_from_streamlit(self.args['source_image'])\n",
        "\n",
        "            images= [self.target_image, self.source_image]\n",
        "            indices_on_page = ['Target image', 'Source image']\n",
        "            st.image(images, width=300, caption=indices_on_page)\n",
        "\n",
        "            # st.image(self.target_image, channels=\"RGB\", caption='Target image')\n",
        "            # st.image(self.source_image, channels=\"RGB\", caption='Source image')\n",
        "\n",
        "        st.sidebar.header('Input some parameters (or using the default is fine)')\n",
        "\n",
        "        st.sidebar.subheader('SDEdit parameters')\n",
        "\n",
        "        self.args['seed'] =  st.sidebar.number_input('Input random seed', min_value=0,\n",
        "                                        value=1234, step=1, format ='%d',\n",
        "                                        )\n",
        "        self.args['sample_step'] = st.sidebar.number_input('Total sampling steps (Number of generated images)', min_value=0, max_value=5,\n",
        "                                        value=1, step=1, format ='%d',\n",
        "                                        )\n",
        "        self.args['t'] = st.sidebar.number_input('Sampling noise scale (Too much is slower, but too little results in an unsatisfying.)', min_value=0, max_value=2000,\n",
        "                                        value=500, step=1, format ='%d',\n",
        "                                        )\n",
        "\n",
        "        # self.args['is_erode_mask'] = int(st.sidebar.checkbox('erode mask')) # erode mask before pass to SDEdit (1) or not (0)\n",
        "        self.args['erode_kernel_size'] = st.sidebar.number_input('erode kernel size', min_value=0, max_value=10,\n",
        "                                        value=7, step=1, format ='%d',\n",
        "                                        )\n",
        "\n",
        "\n",
        "    def create_args(self):\n",
        "        args = dict()\n",
        "        # Image segmentation\n",
        "        args['seg_model_path'] = os.path.join(\"image_segmentation\", \"face_segment_checkpoints_256.pth.tar\")\n",
        "        args['image_size'] = (256,256) # output image size (height, width)\n",
        "        args['input_image_size'] = (256,256) # input image size before segment (height, width)\n",
        "        args['label_config'] = os.path.join(\"image_segmentation\", \"label.yml\") # Path to the label.yml\n",
        "\n",
        "        # SDEdit\n",
        "        args['exp'] = 'exp' # Path for saving running related data.\n",
        "        args['verbose'] = 'info' # 'Verbose level: info | debug | warning | critical'\n",
        "        args['sample'] = True # Whether to produce samples from the model\n",
        "        args['image_folder'] = 'images' # The folder name of samples\n",
        "        args['ni'] = True # No interaction. Suitable for Slurm Job launcher\n",
        "        args['is_erode_mask'] = True\n",
        "        return args\n",
        "\n",
        "    @st.cache\n",
        "    def create_config(_self, config_file_path=os.path.join(\"configs\", \"celeba.yml\")):\n",
        "        # parse config file\n",
        "        with open(config_file_path, 'r') as f:\n",
        "            config = yaml.safe_load(f)\n",
        "        new_config = dict2namespace(config)\n",
        "        return new_config\n",
        "\n",
        "    def read_image_from_streamlit(self, uploaded_file):\n",
        "        # Convert the file to an opencv image.\n",
        "        file_bytes = np.asarray(bytearray(uploaded_file.read()), dtype=np.uint8)\n",
        "        opencv_image = cv2.imdecode(file_bytes, 1)\n",
        "        opencv_image = cv2.cvtColor(opencv_image, cv2.COLOR_BGR2RGB)\n",
        "        return opencv_image\n",
        "\n",
        "    def pipeline(self, ):\n",
        "        segment = face_segment(seg_model_path=self.args['seg_model_path'],\n",
        "                            label_config=self.args['label_config'],\n",
        "                            input_image_size=self.args['input_image_size'])\n",
        "\n",
        "\n",
        "        # infer image segmentation\n",
        "        target_mask = segment.segmenting(image=self.target_image)\n",
        "        source_mask = segment.segmenting(image=self.source_image)\n",
        "\n",
        "        # resize image and mask\n",
        "        target_image = resize_image(self.target_image, self.args['image_size'])\n",
        "        source_image = resize_image(self.source_image, self.args['image_size'])\n",
        "        target_mask = resize_image(target_mask, self.args['image_size'])\n",
        "        source_mask = resize_image(source_mask, self.args['image_size'])\n",
        "\n",
        "        # detect face landmark and transform image\n",
        "        transform_outputs = face_landmark_transform(target_image, target_mask, source_image, source_mask)\n",
        "        transformed_image, transformed_mask = transform_outputs[\"result_image\"], transform_outputs[\"result_mask\"]\n",
        "        transformed_segment = segment.segmenting(image=transformed_image)\n",
        "        # cv2.imwrite('report_images/transformed_mask.png', cv2.cvtColor(transformed_mask, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "        # fill artifacts\n",
        "        filled_image = face_artifact_fill(target_image, target_mask, transformed_image, transformed_mask, transformed_segment)\n",
        "\n",
        "        before_images = [target_mask, source_mask, transformed_image, filled_image]\n",
        "        before_images_captions = ['Target mask', 'Source mask', 'Transformed image', 'Filled image']\n",
        "        st.image(before_images, width=300, caption=before_images_captions, clamp=True)\n",
        "\n",
        "        # SDEdit\n",
        "        sde_mask = transform_outputs['only_fixed_face']\n",
        "        if self.args['is_erode_mask']:\n",
        "            kernel_size = self.args['erode_kernel_size']\n",
        "            kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
        "            sde_mask = cv2.erode(sde_mask, kernel,iterations = 1)\n",
        "\n",
        "\n",
        "        print(\">\" * 80)\n",
        "        logging.info(\"Exp instance id = {}\".format(os.getpid()))\n",
        "        logging.info(\"Config =\")\n",
        "        print(\"<\" * 80)\n",
        "\n",
        "        try:\n",
        "            runner = Diffusion(image_folder=self.args['image_folder'],\n",
        "                            sample_step=self.args['sample_step'],\n",
        "                            total_noise_levels=self.args['t'],\n",
        "                            config=self.config)\n",
        "            self.show_images = runner.image_editing_sample_for_streamlit(filled_image, sde_mask)\n",
        "            images = list(self.show_images.values())\n",
        "            captions = list(self.show_images.keys())\n",
        "            st.image(images, width=100, caption=captions, clamp=True)\n",
        "            for it in range(self.args['sample_step']):\n",
        "                st.image(self.show_images[f'samples_{it}'], width=300, caption=f'Final image {it+1}', clamp=True)\n",
        "\n",
        "        except Exception:\n",
        "                logging.error(traceback.format_exc())\n",
        "\n",
        "        return 0\n",
        "\n",
        "a = app()"
      ],
      "metadata": {
        "id": "CxMG21TUX17m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Set your ngrok auth token\n",
        "ngrok.set_auth_token('1ZUSjbzswWes79aSOOblEYn1is0_2W1XQVFzEHHMzwMw9kdQB')\n",
        "\n",
        "# Start the Streamlit app on port 80\n",
        "!nohup streamlit run app.py --server.port 80 &\n",
        "\n",
        "# Connect ngrok to port 80\n",
        "url = ngrok.connect(80)\n",
        "\n",
        "# Optionally, connect another tunnel to port 8080\n",
        "ngrok.connect(8080)\n",
        "\n",
        "# Get the list of active tunnels\n",
        "tunnels = ngrok.get_tunnels()\n",
        "print(tunnels)"
      ],
      "metadata": {
        "id": "HmsTVgvzTDrZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}